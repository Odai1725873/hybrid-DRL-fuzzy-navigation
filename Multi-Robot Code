mapMatrix = gpuArray(a); % Convert to GPU array
mapScale = 1;
mymap=binaryOccupancyMap(6,6,10)
setOccupancy(mymap,[1 1],a,"grid")
show(mymap)
% Range Sensor Parameters
scanAngles = (-3*pi/8:pi/8:3*pi/8);
maxRange = 3;
lidarNoiseVariance = 0.1^2;
lidarNoiseSeeds =(randi(intmax, size(scanAngles)));
max_x_y = 20;
way = gpuArray([1,4;6,4]);

% Robot Parameters
% Max speed parameters and distance tolerance
maxLinSpeed = 0.4;
maxAngSpeed = 0.4;
add_obs=3; %additional observation for distance

% Initial poses of the robot
initX1 = 1;
initY1 = 1;
initTheta1 = pi/2;
initR1=[initX1 initY1 initTheta1];
finalX1=0;
finalY1=3.85;

initX2 = 1.5;
initY2 = 1;
initTheta2 = pi/2;
initR2=[initX2 initY2 initTheta2];
finalX2=3.45;
finalY2=5.9;

init = gpuArray([initR1; initR2]);

Tfinal = 100;
sampleTime = 0.1;
total_obs=numel(scanAngles)+add_obs;
% Environment Interface
obsInfo = rlNumericSpec([total_obs 1], ...
    "LowerLimit", gpuArray(zeros(total_obs, 1)), ...
    "UpperLimit", gpuArray(ones(total_obs, 1)));

numObservations = obsInfo.Dimension(1);
numActions = 2;

actInfo = rlNumericSpec([numActions 1], ...
    "LowerLimit", gpuArray(-1), ...
    "UpperLimit", gpuArray(1));

% "multiagentsimfuzzy","multiagentsimfuzzy/Agent1","multiagentsimfuzzy/Agent2","multiagentsimfuzzy/Agent3","multiagentsimfuzzy/Agent4"
mdl = "TwoAgentsModel";
blks = mdl + ["/Agent1","/Agent2"];

env = rlSimulinkEnv(mdl,blks,{obsInfo,obsInfo},{actInfo,actInfo});
env.ResetFcn = @(in)ResetFcn(in,scanAngles,maxRange,mapMatrix);
env.UseFastRestart = "on";

%DDPG Agent
for idx=1:2

statePath = [
    featureInputLayer(numObservations, "Normalization","none","Name","State")
    fullyConnectedLayer(50,"Name","CriticStateFC1")
    reluLayer("Name","CriticRelu1")
    fullyConnectedLayer(25,"Name","CriticStateFC2")];
actionPath = [
    featureInputLayer(numActions,"Normalization","none","Name","Action")
    fullyConnectedLayer(25,"Name","CriticActionFC1")];
commonPath = [
    additionLayer(2,"Name","add")
    reluLayer("Name","CriticCommonRelu")
    fullyConnectedLayer(1,"Name","CriticOutput")];

criticNetwork = layerGraph();
criticNetwork = addLayers(criticNetwork,statePath);
criticNetwork = addLayers(criticNetwork,actionPath);
criticNetwork = addLayers(criticNetwork,commonPath);
criticNetwork = connectLayers(criticNetwork,"CriticStateFC2","add/in1");
criticNetwork = connectLayers(criticNetwork,"CriticActionFC1","add/in2");
criticNetwork = dlnetwork(criticNetwork);

criticOptions = rlOptimizerOptions("LearnRate",1e-3,"L2RegularizationFactor",1e-4,"GradientThreshold",1);
critic(idx) = rlQValueFunction(criticNetwork,obsInfo,actInfo,"ObservationInputNames","State","ActionInputNames","Action");

actorNetwork = [
    featureInputLayer(numObservations,"Normalization","none","Name","State")
    fullyConnectedLayer(50,"Name","actorFC1")
    reluLayer("Name","actorReLU1")
    fullyConnectedLayer(50, "Name","actorFC2")
    reluLayer("Name","actorReLU2")
    fullyConnectedLayer(2, "Name","actorFC3")
    tanhLayer("Name","Action")];
actorNetwork = dlnetwork(actorNetwork);

actorOptions = rlOptimizerOptions("LearnRate",1e-4,"L2RegularizationFactor",1e-4,"GradientThreshold",1);
actor(idx) = rlContinuousDeterministicActor(actorNetwork,obsInfo,actInfo);
end

% Create DDPG agent object
agentOpts = rlDDPGAgentOptions(...
    "SampleTime",sampleTime,...
    "ActorOptimizerOptions",actorOptions,...
    "CriticOptimizerOptions",criticOptions,...
    "DiscountFactor",0.995, ...
    "MiniBatchSize",128, ...
    "ExperienceBufferLength",1e6); 

agentOpts.NoiseOptions.Variance = 0.1;
agentOpts.NoiseOptions.VarianceDecayRate = 1e-5;

%%Create the rlDDPGAgent object.
obstacleAvoidanceAgent1 = rlDDPGAgent(actor(1),critic(1),agentOpts);
obstacleAvoidanceAgent2 = rlDDPGAgent(actor(2),critic(2),agentOpts);

%open_system(mdl + ["/Agent1","/Agent2"])


% Train Agent
maxEpisodes = 4000;
maxSteps = 200;
trainOpts = rlMultiAgentTrainingOptions(...
    "AgentGroups","auto",...
    "LearningStrategy","decentralized",...
    "MaxEpisodes",maxEpisodes, ...
    "MaxStepsPerEpisode",maxSteps, ...
    "ScoreAveragingWindowLength",50, ...
    "StopTrainingCriteria","EpisodeCount", ...
    "StopTrainingValue",4000, ...
    "Verbose", true, ...
    "Plots","training-progress");
